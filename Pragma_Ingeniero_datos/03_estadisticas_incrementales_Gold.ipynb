{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "854c6f3f-ba46-47d7-b779-64b2a0b55655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parámetros y referencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6759d9b0-70a3-4f31-9e14-ff57dfc09c2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# [03][1] Parámetros de métricas y referencias a tablas\n",
    "dbutils.widgets.text(\"CATALOGO\",      \"workspace\")\n",
    "dbutils.widgets.text(\"ESQUEMA_PLATA\", \"silver_mb\")\n",
    "dbutils.widgets.text(\"ESQUEMA_ORO\",   \"gold_mb\")\n",
    "dbutils.widgets.text(\"PREFIJO_TABLA\", \"mb_\")\n",
    "dbutils.widgets.dropdown(\"MODO_TRIGGER\",\"once\", [\"once\",\"availableNow\"])\n",
    "\n",
    "CATALOGO     = dbutils.widgets.get(\"CATALOGO\")\n",
    "ESQ_PLATA    = dbutils.widgets.get(\"ESQUEMA_PLATA\")\n",
    "ESQ_ORO      = dbutils.widgets.get(\"ESQUEMA_ORO\")\n",
    "PREFIJO      = dbutils.widgets.get(\"PREFIJO_TABLA\")\n",
    "MODO_TRIGGER = dbutils.widgets.get(\"MODO_TRIGGER\")\n",
    "\n",
    "T_PLATA   = f\"{CATALOGO}.{ESQ_PLATA}.{PREFIJO}events_silver\"\n",
    "T_LOG     = f\"{CATALOGO}.{ESQ_ORO}.{PREFIJO}ingestion_log\"\n",
    "T_RUNNING = f\"{CATALOGO}.{ESQ_ORO}.{PREFIJO}running_stats\"\n",
    "CHK_ORO   = \"/tmp/_chk_oro\"   # en productivo: apuntaría a storage gobernado\n",
    "\n",
    "display({\"T_PLATA\": T_PLATA, \"T_LOG\": T_LOG, \"T_RUNNING\": T_RUNNING, \"CHK_ORO\": CHK_ORO})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e82e9cb-9fbd-4fa9-acc5-e56700a92e7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Función foreachBatch (agregación por micro-batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f044ac9d-a1e3-4ec8-95ff-7487f00efe70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# [03][2] Lógica foreachBatch:\n",
    "# Calcula métricas SOLO del micro-batch y actualiza:\n",
    "# - T_LOG (auditoría por archivo)\n",
    "# - T_RUNNING (acumulado en una sola fila id=1)\n",
    "\n",
    "from pyspark.sql.functions import count, sum as ssum, min as smin, max as smax, lit\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def actualizar_metricas(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "\n",
    "    m = (batch_df.agg(\n",
    "            count(lit(1)).alias(\"rows\"),\n",
    "            ssum(\"price\").alias(\"sum_price\"),\n",
    "            smin(\"price\").alias(\"min_price\"),\n",
    "            smax(\"price\").alias(\"max_price\"),\n",
    "            smin(\"ts\").alias(\"min_ts\"),\n",
    "            smax(\"ts\").alias(\"max_ts\")\n",
    "         ).collect()[0])\n",
    "\n",
    "    filas      = int(m[\"rows\"])\n",
    "    suma_p     = float(m[\"sum_price\"])\n",
    "    min_p      = float(m[\"min_price\"])\n",
    "    max_p      = float(m[\"max_price\"])\n",
    "    min_ts     = m[\"min_ts\"]\n",
    "    max_ts     = m[\"max_ts\"]\n",
    "\n",
    "    # Trazabilidad del micro-batch (intento tomar el _batch_id que viene de Plata)\n",
    "    if \"_batch_id\" in batch_df.columns:\n",
    "        bid = batch_df.select(\"_batch_id\").limit(1).collect()[0][\"_batch_id\"]\n",
    "    else:\n",
    "        bid = f\"batch_{batch_id}\"\n",
    "\n",
    "    # 1) Inserto auditoría en T_LOG\n",
    "    (spark.createDataFrame(\n",
    "        [(bid, filas, suma_p, min_p, max_p, min_ts, max_ts)],\n",
    "        \"batch_id STRING, rows_loaded BIGINT, sum_price DOUBLE, min_price DOUBLE, max_price DOUBLE, min_ts TIMESTAMP, max_ts TIMESTAMP\"\n",
    "     ).withColumn(\"processed_at\", lit(None).cast(\"timestamp\"))\n",
    "     .write.format(\"delta\").mode(\"append\").saveAsTable(T_LOG))\n",
    "\n",
    "    # 2) Actualizo acumulado en T_RUNNING mediante MERGE (sin recalcular histórico)\n",
    "    rt = DeltaTable.forName(spark, T_RUNNING)\n",
    "    (rt.alias(\"t\").merge(\n",
    "        spark.createDataFrame(\n",
    "            [(1, filas, suma_p, min_p, max_p, min_ts, max_ts)],\n",
    "            \"id INT, d_rows BIGINT, d_sum DOUBLE, d_min DOUBLE, d_max DOUBLE, d_min_ts TIMESTAMP, d_max_ts TIMESTAMP\"\n",
    "        ).alias(\"d\"),\n",
    "        \"t.id = d.id\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"row_count\":  \"t.row_count + d.d_rows\",\n",
    "        \"sum_price\":  \"t.sum_price + d.d_sum\",\n",
    "        \"min_price\":  \"CASE WHEN t.min_price IS NULL OR d.d_min < t.min_price THEN d.d_min ELSE t.min_price END\",\n",
    "        \"max_price\":  \"CASE WHEN t.max_price IS NULL OR d.d_max > t.max_price THEN d.d_max ELSE t.max_price END\",\n",
    "        \"min_ts\":     \"CASE WHEN t.min_ts IS NULL OR d.d_min_ts < t.min_ts THEN d.d_min_ts ELSE t.min_ts END\",\n",
    "        \"max_ts\":     \"CASE WHEN t.max_ts IS NULL OR d.d_max_ts > t.max_ts THEN d.d_max_ts ELSE t.max_ts END\",\n",
    "        \"updated_at\": \"current_timestamp()\n",
    "    }).whenNotMatchedInsert(values={\n",
    "        \"id\":         \"d.id\",\n",
    "        \"row_count\":  \"d.d_rows\",\n",
    "        \"sum_price\":  \"d.d_sum\",\n",
    "        \"min_price\":  \"d.d_min\",\n",
    "        \"max_price\":  \"d.d_max\",\n",
    "        \"min_ts\":     \"d.d_min_ts\",\n",
    "        \"max_ts\":     \"d.d_max_ts\",\n",
    "        \"updated_at\": \"current_timestamp()\n",
    "    }).execute())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db0e688e-da7a-47ec-9372-13e221d04569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Lanzar foreachBatch con trigger CE-safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eda6700-780a-45de-95a8-5f1809fce0e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# [03][3] Ejecución CE-safe:\n",
    "# Uso trigger once/availableNow para procesar lo pendiente y terminar (Community-friendly).\n",
    "\n",
    "stream = (spark.readStream.table(T_PLATA)\n",
    "          .writeStream\n",
    "          .foreachBatch(actualizar_metricas)\n",
    "          .option(\"checkpointLocation\", CHK_ORO)\n",
    "          .outputMode(\"update\"))\n",
    "\n",
    "if MODO_TRIGGER == \"availableNow\":\n",
    "    q = stream.trigger(availableNow=True).start()\n",
    "else:\n",
    "    q = stream.trigger(once=True).start()\n",
    "\n",
    "q.awaitTermination()\n",
    "print(\"[INFO] Métricas acumuladas actualizadas (sin reescaneo de histórico).\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5978285600230572,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_estadisticas_incrementales_Gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
